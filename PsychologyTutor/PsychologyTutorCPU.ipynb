{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14HxvJpVMFHaQl1Q3WxxGMfRweXD_mGhm","timestamp":1741956995965},{"file_id":"1M5jfERFJniGeVIC4aL0Rv1ZXw46WZx-F","timestamp":1741856202486},{"file_id":"1qjKJH_1_5bYA4SFbFbYwKwyBd0I22Fno","timestamp":1741794708387},{"file_id":"17AJ0nQYItGmFbUtnCauI2yRBWCiDgCwI","timestamp":1741737278264}],"authorship_tag":"ABX9TyMrqZHPgaar9aSYLboTWQjy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Install the required packages:"],"metadata":{"id":"9P9-uXrb3t1-"}},{"cell_type":"code","source":["!pip install faiss-cpu transformers accelerate torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fas2AqiBhhS0","executionInfo":{"status":"ok","timestamp":1741957441349,"user_tz":0,"elapsed":136070,"user":{"displayName":"Annie Bond","userId":"07581104602593571092"}},"outputId":"7de002a4-24bf-4e83-e63e-d1d1027dfbff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import pipeline\n","import faiss\n","import pandas as pd\n","import numpy as np\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"Xe3BE91xvRuJ","executionInfo":{"status":"ok","timestamp":1741957631604,"user_tz":0,"elapsed":28429,"user":{"displayName":"Annie Bond","userId":"07581104602593571092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def embed_question(question, model_name):\n","    encoder = SentenceTransformer(model_name)\n","    embedding = encoder.encode([question])\n","    return embedding\n","\n","def retrieve_indexes_from_faiss(embedding, k, database_name):\n","    # convert embeddings numpy float32 format for faiss:\n","    embedding_array = np.array(embedding, dtype=np.float32)\n","    index = faiss.read_index(database_name)\n","\n","    # search faiss using query embedding:\n","    distances, indexes = index.search(embedding_array, k)\n","    indexes = indexes[0]\n","    distances = distances[0]\n","    print(\"indexes: \", indexes)\n","    print(\"distances: \", distances)\n","    return indexes\n","\n","def retrieve_relevant_context_from_csv(indexes, csv_filename):\n","    df = pd.read_csv(csv_filename)\n","    context = \"\"\n","    for index in indexes:\n","        # get the text chunk where the id matches and add to context\n","        result = df.loc[df[\"id\"] == index, \"text_chunk\"]\n","        if not result.empty:\n","            context = context + \"\\n\" + result.values[0]\n","        else:\n","            print(\"no text chunk for id\", index)\n","\n","    print(\"retrieved context: \", context)\n","    return context\n","\n","def construct_prompt(question, context):\n","    prompt_start = \"Answer the following question: \"\n","    promt_context = \" Context: \" + context\n","    prompt = prompt_start + question + promt_context\n","    return prompt\n","\n","\n","question= \"what is the multi-store memory model?\"\n","embedding = embed_question(question, \"BAAI/bge-small-en-v1.5\")\n","indexes = retrieve_indexes_from_faiss(embedding, 5, \"faiss.index\")\n","indexes = np.sort(indexes)\n","context = retrieve_relevant_context_from_csv(indexes, \"text_chunks.csv\")\n","\n","# get the LM:\n","model_name = \"HuggingFaceTB/SmolLM-135M\"\n","model = pipeline(\"text-generation\", model=model_name)\n","\n","prompt = construct_prompt(question, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVJITLnWu3tl","executionInfo":{"status":"ok","timestamp":1741960733836,"user_tz":0,"elapsed":1381,"user":{"displayName":"Annie Bond","userId":"07581104602593571092"}},"outputId":"f8284379-fde8-4f6a-e77c-10f62aafcf49"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["indexes:  [24 32 27 31 26]\n","distances:  [0.3739136  0.51471996 0.5285149  0.63307667 0.7145467 ]\n","retrieved context:  \n","Atkinson and Shiffrin (1968) devised the multi-store model (MSM) of memory. It is a cognitive approach that explains memory as information passing through a series of 3 storage systems: the sensory register, then short-term memory, and then long-term memory.\n","Long-term memory lasts anywhere from more than 30 seconds to an entire lifetime. There are several different types of long-term memory (see below), but all long-term memories will have originally passed through both the sensory register and short-term memory.\n","Once a long-term memory is stored, it can be retrieved and temporarily transferred to short term memory and manipulated (this retrieval process may also improve the duration of the memory). An example of retrieval would be remembering a pleasant experience from your childhood and thinking about how you felt that day.\n","The working memory model (WMM) was developed by Baddeley and Hitch (1974) and builds on the multi-store model – in particular, the MSM’s model of short-term memory. Rather than replacing the MSM, the WMM is better understood as a more detailed description of the MSM’s short-term memory component.\n","Whereas the MSM describes short-term memory as a single store, the WMM divides short-term memory into 4 separate components: the central executive, phonological loop, visuo-spatial sketchpad, and the episodic buffer. These components serve different functions and hold different types of information, which are actively worked upon as the individual thinks.\n","The visuo-spatial sketchpad is the mind’s inner eye – it stores visual and spatial information. Information within the visuo-spatial sketchpad is coded as mental pictures.\n","The episodic buffer is a temporary store for information coded in all forms (auditory, visual, etc.).\n","The original WMM did not include the episodic buffer. But because the central executive has no storage capacity, and the phonological loop and visuo-spatial sketchpad are only able to store and process specific types of information, the introduction of the episodic buffer in 2000 explains how people can combine and store general information from the various components of short- and long-term memory.\n","For example, the working memory of a story will likely contain visual, semantic, and chronological information (i.e. the order of events). These different types of information would be combined in the episodic buffer to form a coherent story in short-term memory.\n","The multi-store model of long-term memory (LTM) explains it as one single store. However, this is overly simplistic, as research suggests there are many different types of long-term memory.\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"code","source":["# generate the response:\n","output = model(prompt, max_length = 1000, early_stopping=False)\n","output_text = output[0][\"generated_text\"].replace(prompt, \"\")\n","print(output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1Piy1bK8vJi","executionInfo":{"status":"ok","timestamp":1741960859431,"user_tz":0,"elapsed":70181,"user":{"displayName":"Annie Bond","userId":"07581104602593571092"}},"outputId":"47751227-029a-489d-a94f-1fb9c34a66ba"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The multi-store model of long-term memory (MSM) is a cognitive approach that explains long-term memory as information passing through a series of 3 storage systems: the sensory register, short-term memory, and long-term memory.\n","Long-term memory lasts anywhere from 30 seconds to an entire lifetime. There are several different types of long-term memories (see below), but all long-term memories will have originally passed through both the sensory register and short-term memory.\n","Once a long-term memory is stored, it can be retrieved and temporarily transferred to short term memory and manipulated (this retrieval process may also improve the duration of the memory). An example of retrieval would be remembering a pleasant experience from your childhood and thinking about how you felt that day.\n","The working memory model (WMM) was developed by Baddeley and Hitch (1974) and builds on the multi-store model – in particular, the MSM’s model of short-term memory. Rather than replacing the MSM, the WMM is better understood as a more detailed description of the MSM’s short-term memory component.\n","Whereas the MSM describes short-term memory as a single store, the WMM divides short-term memory into 4 separate components: the central executive, phonological loop, visuo-spatial sketchpad, and the episodic buffer. These components serve different functions and hold different types of information, which are actively worked upon as the individual thinks.\n","The visuo-spatial sketchpad is the mind’s inner eye – it stores visual and spatial information. Information within the visuo-spatial sketchpad is coded as mental pictures.\n","The episodic buffer is a temporary store for information coded in all forms (auditory, visual, etc.). Information within the episodic buffer is coded as mental pictures.\n","The original WMM did not include the episodic buffer. But because the central executive has no storage capacity, and the phonological loop and visuo-spatial sketchpad are only able to\n"]}]}]}